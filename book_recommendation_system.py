# -*- coding: utf-8 -*-
"""book-recommendation-system.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tJo_85MkamvDMgGLW0SSLrxpOXTRzM1D

# Book Recommendation menggunakan Collaborative Filtering

## Penyiapan Data

### Import Library
"""

from google.colab import files
import os
import zipfile
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error
import seaborn as sns
import scipy.sparse as sp

"""### Menyiapkan Kredensial Kaggle


"""

# Upload kaggle.json

uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}"'.format(
      name=fn))

# Ubah permission file
!chmod 600 /content/kaggle.json

# Setup Kaggle environment
os.environ['KAGGLE_CONFIG_DIR'] = "/content"

"""Informasi Dataset:

Jenis | Keterangan
--- | ---
Title | Book Recommendation Dataset
Source | [Kaggle](https://www.kaggle.com/arashnic/book-recommendation-dataset)
Maintainer | [Möbius](https://www.kaggle.com/arashnic)
License | [CC0: Public Domain](https://creativecommons.org/publicdomain/zero/1.0/)
Usability | 10.0
"""

# Download Dataset
!kaggle datasets download -d arashnic/book-recommendation-dataset

# melakukan ekstraksi pada file zip
local_zip = 'book-recommendation-dataset.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/content/book-recommendation-dataset/')
zip_ref.close()

# Menghapus berkas zip yang sudah tidak diperlukan
!rm book-recommendation-dataset.zip

"""## Data Understanding

Dataset ini terdiri dari tiga berkas CSV, yaitu ```Books.csv```, ```Ratings.csv```, dan ```Users.csv```.
Langkah selanjutnya, kita akan membuka masing-masing berkas tersebut menggunakan bantuan pustaka ```pandas``` untuk melihat isi datanya.
"""

# Load dataset
books = pd.read_csv('book-recommendation-dataset/Books.csv')
ratings = pd.read_csv('book-recommendation-dataset/Ratings.csv')
users = pd.read_csv('book-recommendation-dataset/Users.csv')

"""### Books

Berikut ini adalah isi dari `Books.csv`
"""

books

books.info()

"""Dari keluaran di atas dapat diketahui bahwa berkas `Books.csv` memuat data-data buku yang terdiri dari 271360 baris dan memiliki 8 kolom, diantaranya adalah :  

- `ISBN` : berisi kode ISBN dari buku  
- `Book-Title` : berisi judul buku
- `Book-Author` : berisi penulis buku
- `Year-Of-Publication` : tahun terbit buku  
- `Publisher` : penerbit buku  
- `Image-URL-S` : URL menuju gambar buku berukuran kecil
- `Image-URL-M` : URL menuju gambar buku berukuran sedang
- `Image-URL-L` : URL menuju gambar buku berukuran besar

### Ratings

Berikut ini adalah isi dari berkas `Ratings.csv`
"""

ratings

ratings.groupby('Book-Rating').count()

rating_counter = ratings.groupby('Book-Rating').count()
plt.figure(figsize=(10,5))
plt.title('Jumlah Rating Buku yang Diberikan Pengguna')
plt.xlabel('Rating')
plt.ylabel('Jumlah Buku')
plt.bar(rating_counter.index, rating_counter['ISBN'])
plt.grid(True)
plt.show()

"""Pada visualisasi data di atas dapat diketahui bahwa data tidak seimbang dan banyak pengguna yang memberikan rating 0."""

ratings.info()

ratings.describe().apply(lambda s: s.apply(lambda x: format(x, 'f')))

"""Berdasarkan hasil di atas, dapat disimpulkan bahwa berkas ```Ratings.csv``` berisi data penilaian buku dari para pengguna. Dataset ini terdiri dari 1.149.780 baris dan memiliki 3 kolom, yaitu:  
 - `User-ID` : menyimpan ID unik pengguna
 - `ISBN` : berisi kode ISBN buku yang dinilai oleh pengguna
 - `Book-Rating` : berisi nilai rating yang diberikan oleh pengguna berkisar antara 0-10

### Users

Berikut ini adalah isi dari `Users.csv`
"""

users

users.info()

users.describe()

"""Dari hasil di atas dapat diketahui bahwa berkas `Users.csv` memuat data pengguna. Data ini terdiri dari 278858 baris dan memiliki 3 kolom, yaitu :

- `User-ID` : berisi ID unik pengguna
- `Location` : berisi data lokasi pengguna
- `Age` : berisi data usia pengguna

### EDA Univariatae
"""

# Distribusi rating buku
plt.figure(figsize=(8,5))
sns.countplot(x='Book-Rating', data=ratings)
plt.title('Distribusi Book-Rating')
plt.xlabel('Book-Rating')
plt.ylabel('Jumlah')
plt.show()

# Distribusi usia pengguna
plt.figure(figsize=(8,5))
sns.histplot(users['Age'], bins=30, kde=True)
plt.title('Distribusi Usia Pengguna')
plt.xlabel('Age')
plt.ylabel('Frekuensi')
plt.show()

# 3. Distribusi penerbit terbanyak (top 20)
top_publishers = books['Publisher'].value_counts().head(20)
plt.figure(figsize=(12,6))
sns.barplot(x=top_publishers.values, y=top_publishers.index)
plt.title('20 Penerbit dengan Jumlah Buku Terbanyak')
plt.xlabel('Jumlah Buku')
plt.ylabel('Penerbit')
plt.show()

"""- Book-Rating: Data rating tidak seimbang di rating (0). Jika di nilai dari rating (1-10) mayoritas pengguna memberi rating tinggi (6–10), menunjukkan kepuasan tinggi terhadap buku.

- Usia Pengguna: Didominasi oleh usia 20–40 tahun; terdapat outlier hingga usia 250. Tapi fitur ini nantinya tidak digunakan dalam training model.

- Penerbit Buku: Harlequin paling produktif, diikuti Silhouette dan Pocket — dominasi penerbit besar terlihat jelas.

### EDA Multivariate
"""

# 1. Distribusi Jumlah Rating per User
user_rating_counts = ratings['User-ID'].value_counts()

plt.figure(figsize=(10, 5))
sns.histplot(user_rating_counts, bins=50, kde=False)
plt.title('Distribusi Jumlah Rating per User')
plt.xlabel('Jumlah Rating')
plt.ylabel('Jumlah User')
plt.xlim(0, 100)  # Batasi untuk visualisasi
plt.grid(True, linestyle='--', alpha=0.5)
plt.show()

# 2. Top 20 Buku dengan Jumlah Rating Terbanyak (berdasarkan judul)
ratings_with_titles = ratings.merge(books[['ISBN', 'Book-Title']], on='ISBN', how='left')
top_books_title = ratings_with_titles['Book-Title'].value_counts().head(20)

plt.figure(figsize=(12, 6))
sns.barplot(x=top_books_title.values, y=top_books_title.index, color='steelblue')
plt.title('Top 20 Buku dengan Jumlah Rating Terbanyak')
plt.xlabel('Jumlah Rating')
plt.ylabel('Judul Buku')
plt.grid(axis='x', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

# 3. Scatter Plot: User-ID vs Book-Rating (Sample)
plt.figure(figsize=(10, 6))
sns.scatterplot(x='User-ID', y='Book-Rating', data=ratings.sample(10000), alpha=0.2)
plt.title('Sebaran Rating oleh User')
plt.xlabel('User-ID')
plt.ylabel('Book-Rating')
plt.grid(True, linestyle='--', alpha=0.5)
plt.show()

"""Berdasarkan visualisasi data, dapat disimpulkan bahwa mayoritas pengguna hanya memberikan sedikit rating, menunjukkan tingkat partisipasi yang rendah. Buku yang paling banyak diberi rating adalah Wild Animus, mengindikasikan popularitas atau promosi besar-besaran terhadap buku tersebut. Selain itu, distribusi rating cenderung tinggi (nilai 8–10), yang menunjukkan adanya bias positif dari pengguna dalam memberikan penilaian.

## Data Preparation

Sebelum melakukan pemodelan, data perlu melewati proses persiapan terlebih dahulu. Berikut adalah langkah-langkah yang dilakukan dalam tahap persiapan data.

### Handling Imbalanced Data

Karena sebelumnya diketahui bahwa data rating tidak seimbang, pada tahap ini saya melakukan penghapusan data dengan rating 0.
"""

ratings.drop(ratings[ratings["Book-Rating"] == 0].index, inplace=True)

"""Berikut ini adalah jumlah data setelah di-drop"""

ratings.shape

ratings

rating_counter = ratings.groupby('Book-Rating').count()
plt.figure(figsize=(10,5))
plt.title('Jumlah Rating Buku yang Diberikan Pengguna')
plt.xlabel('Rating')
plt.ylabel('Jumlah Buku')
plt.bar(rating_counter.index, rating_counter['ISBN'])
plt.grid(True)
plt.show()

"""### Encoding Data

Proses encoding dilakukan untuk mengonversi ``User-ID`` dan ``ISBN`` menjadi indeks berbentuk bilangan bulat (integer).
"""

# Mengubah User-ID menjadi list unik tanpa duplikasi
user_ids = ratings['User-ID'].unique().tolist()

# Melakukan encoding User-ID ke angka (integer)
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}

# Membuat mapping kebalikan dari angka ke User-ID asli
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}

# Mengubah ISBN menjadi list tanpa nilai yang sama
isbn_list = ratings['ISBN'].unique().tolist()

# Melakukan encoding ISBN
isbn_to_isbn_encoded = {x: i for i, x in enumerate(isbn_list)}

# Melakukan proses encoding angka ke ISBN
isbn_encoded_to_isbn = {i: x for i, x in enumerate(isbn_list)}

"""Setelah encoding selesai, hasilnya dimapping ke dataframe ``ratings``"""

# Mapping User-ID ke kolom 'user' dengan indeks encoded
ratings['user'] = ratings['User-ID'].map(user_to_user_encoded)

# Mapping ISBN ke kolom 'book' dengan indeks encoded
ratings['book'] = ratings['ISBN'].map(isbn_to_isbn_encoded)

ratings

ratings.info()

"""### Randomize Dataset

Proses ini dilakukan untuk mengacak urutan data sehingga distribusi datanya menjadi acak dan tidak berurutan. Tujuannya adalah agar model yang dibangun tidak terpengaruh oleh pola urutan data asli, sehingga hasil pelatihan menjadi lebih representatif dan mengurangi risiko bias akibat urutan data.
"""

# Mengacak dataset
df = ratings.sample(frac=1, random_state=42)
df

"""### Data Standardization and Splitting

Setelah data diacak, dataset kemudian dibagi menjadi dua bagian, yaitu data pelatihan sebanyak 80% dan data validasi sebanyak 20%.

Selain itu, nilai rating yang awalnya berada pada rentang 0 hingga 10 distandarisasi menjadi rentang 0 hingga 1 untuk mempermudah dan meningkatkan efisiensi proses pelatihan model.
"""

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)

# Mendapatkan jumlah resto
num_isbn = len(isbn_encoded_to_isbn)
print(num_isbn)

# Mengubah rating menjadi nilai float
df['Book-Rating'] = df['Book-Rating'].values.astype(np.float32)

# Nilai minimum Book-Rating
min_rating = min(df['Book-Rating'])

# Nilai maksimal Book-Rating
max_rating = max(df['Book-Rating'])

print('Number of User: {}, Number of ISBN: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_isbn, min_rating, max_rating
))

# Membuat variabel x untuk mencocokkan data user dan book menjadi satu value
x = df[['user', 'book']].values

# Membuat variabel y untuk membuat rating dari hasil
y = df['Book-Rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""## Modelling

### Membuat Kelas RecommenderNet

Membangun model rekomendasi berbasis matrix factorization (faktorisasi matriks) menggunakan embedding untuk user dan item (dalam hal ini, ISBN buku), mirip dengan pendekatan pada model-model seperti Collaborative Filtering.
"""

# Definisi model rekomendasi
class RecommenderNet(tf.keras.Model):
    def __init__(self, num_users, num_isbn, embedding_size, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)
        self.num_users = num_users
        self.num_isbn = num_isbn
        self.embedding_size = embedding_size

        self.user_embedding = layers.Embedding(
            input_dim=num_users,
            output_dim=embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-6)
        )
        self.user_bias = layers.Embedding(num_users, 1)

        self.isbn_embedding = layers.Embedding(
            input_dim=num_isbn,
            output_dim=embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-6)
        )
        self.isbn_bias = layers.Embedding(num_isbn, 1)

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])
        user_bias = self.user_bias(inputs[:, 0])
        isbn_vector = self.isbn_embedding(inputs[:, 1])
        isbn_bias = self.isbn_bias(inputs[:, 1])

        dot_user_isbn = tf.reduce_sum(user_vector * isbn_vector, axis=1, keepdims=True)
        x = dot_user_isbn + user_bias + isbn_bias

        return tf.nn.sigmoid(x)

"""- Struktur Kode & Penjelasan
  - `class RecommenderNet(tf.keras.Model)`: Mendefinisikan kelas model kustom turunan dari tf.keras.Model.
  - `def __init__(self, num_users, num_isbn, embedding_size, **kwargs)`: Konstruktor untuk menginisialisasi parameter dan layer.

  - `super(RecommenderNet, self).__init__(**kwargs)`
Memanggil konstruktor dari kelas induk `(tf.keras.Model)`.

  - `self.user_embedding = layers.Embedding`(...)
Layer embedding untuk merepresentasikan setiap user dalam bentuk vektor berdimensi embedding_size.

  - `self.user_bias = layers.Embedding(num_users, 1)`
Embedding 1-dimensi untuk menyimpan bias per user (semacam preferensi umum user terhadap rating).

  - `self.isbn_embedding = layers.Embedding(...)`
Layer embedding untuk merepresentasikan setiap buku (berdasarkan ISBN).

  - `self.isbn_bias = layers.Embedding(num_isbn, 1)`
Bias per buku menangkap apakah sebuah buku secara umum cenderung mendapat rating tinggi atau rendah.

  - `user_vector = self.user_embedding(inputs[:, 0])` , `user_bias = self.user_bias(inputs[:, 0])` , `isbn_vector = self.isbn_embedding(inputs[:, 1])` , `isbn_bias = self.isbn_bias(inputs[:, 1])` :
Mengambil embedding dan bias untuk user dan ISBN dari input.

  - `dot_user_isbn = tf.reduce_sum(user_vector * isbn_vector, axis=1, keepdims=True)`: Menghitung dot product antara embedding user dan buku — nilai ini mencerminkan tingkat kesesuaian user terhadap buku tersebut.

  - `x = dot_user_isbn + user_bias + isbn_bias` : Menambahkan bias user dan bias ISBN ke hasil dot product.

  - `return tf.nn.sigmoid(x)` : Menggunakan sigmoid untuk menormalkan output ke rentang 0–1 — cocok untuk prediksi rating terstandarisasi (misal 0–1), atau bisa disesuaikan nanti ke skala 1–10.

### Melatih Model
"""

best_model = RecommenderNet(num_users=num_users, num_isbn=num_isbn, embedding_size=12)

best_model.compile(
    loss=tf.keras.losses.MeanSquaredError(),
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

history = best_model.fit(
    x_train,
    y_train,
    epochs=20,
    batch_size=200,
    validation_data=(x_val, y_val),
    callbacks=[
        tf.keras.callbacks.EarlyStopping(
            monitor='val_root_mean_squared_error',
            patience=3,
            restore_best_weights=True
        )
    ]
)

"""Model berhenti dilatih pada epoch ke-19 karena tidak ada peningkatan signifikan pada metrik validasi selama beberapa epoch terakhir. Dengan nilai :

- RMSE : 0.1288
-val-loss: 0.378
-val_RMSE : 0.1929

## Evaluasi
"""

loss, rmse = best_model.evaluate(x_val, y_val)
print(f"Validation RMSE: {rmse}")

"""Nilai RMSE yang dicapai pada data validasi sebesar 0.1925, menunjukkan bahwa prediksi model terhadap rating cukup akurat dan stabil."""

# Visualisasi metrik
plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('Model RMSE per Epoch')
plt.ylabel('RMSE')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper right')
plt.grid(True)
plt.show()

"""Model menunjukkan peningkatan performa selama pelatihan, ditandai dengan:

- RMSE pada data training terus menurun secara konsisten.

- RMSE pada data validasi menurun di awal dan stabil mendekati 0.19, tanpa overfitting yang jelas.

Artinya, model belajar dengan baik dan generalisasi cukup baik terhadap data yang belum pernah dilihat. Model berhenti secara otomatis di epoch ke-19 karena tidak ada peningkatan signifikan, sesuai pengaturan EarlyStopping.

## Mendapatkan Rekomendasi
"""

books_data = books  # Alias supaya beda dari nama awal
ratings_data = pd.read_csv('book-recommendation-dataset/Ratings.csv')

# Ambil 1 user secara acak
chosen_user_id = ratings_data['User-ID'].sample(1).iloc[0]
user_ratings = ratings_data[ratings_data['User-ID'] == chosen_user_id]

# Buku yang belum dibaca user tersebut
unread_isbns = books_data[~books_data['ISBN'].isin(user_ratings['ISBN'])]['ISBN']
unread_encoded = [isbn_to_isbn_encoded[isbn] for isbn in unread_isbns if isbn in isbn_to_isbn_encoded]

# Buat array input untuk prediksi
encoded_user_id = user_to_user_encoded[chosen_user_id]
prediction_input = np.hstack(
    ([[encoded_user_id]] * len(unread_encoded), np.array(unread_encoded).reshape(-1, 1))
)

# Prediksi rating dari model
predicted_scores = best_model.predict(prediction_input).flatten()
top_indices = predicted_scores.argsort()[-10:][::-1]

recommended_isbns = [isbn_encoded_to_isbn[unread_encoded[i]] for i in top_indices]

# Tampilkan hasil
print(f"Rekomendasi untuk User ID: {chosen_user_id}")
print("=" * 30)

print("\nBuku dengan rating tertinggi dari user:")
print("-" * 40)
top_user_books = user_ratings.sort_values(by='Book-Rating', ascending=False).head(5)['ISBN']
top_books_info = books_data[books_data['ISBN'].isin(top_user_books)]

for _, row in top_books_info.iterrows():
    print(f"{row['Book-Title']} - {row['Book-Author']}")

print("\nRekomendasi Buku Terbaik untuk User Ini:")
print("-" * 40)
recommended_info = books_data[books_data['ISBN'].isin(recommended_isbns)]

for _, row in recommended_info.iterrows():
    print(f"{row['Book-Title']} - {row['Book-Author']}")

"""### **Kesimpulan**

Model sistem rekomendasi buku berbasis deep learning berhasil dikembangkan dan diimplementasikan menggunakan data interaksi pengguna dan buku. Berdasarkan hasil evaluasi dan pengujian, model mampu memberikan rekomendasi yang relevan secara personal bagi pengguna, termasuk menampilkan daftar buku yang belum pernah dibaca namun diprediksi akan disukai.

Model ini telah menunjukkan performa yang baik dengan nilai evaluasi RMSE yang cukup rendah, menandakan akurasi prediksi yang tinggi. Rekomendasi yang dihasilkan juga menunjukkan konsistensi terhadap preferensi pengguna berdasarkan histori rating sebelumnya.

Sistem yang telah dikembangkan ini memiliki potensi besar untuk diimplementasikan lebih lanjut sebagai **aplikasi nyata** dalam platform digital, seperti toko buku online atau aplikasi perpustakaan.

Meski demikian, **pengembangan lanjutan masih diperlukan**, seperti:

* Integrasi fitur tambahan (misalnya genre, sinopsis, metadata buku),
* Penggabungan pendekatan hybrid (content-based + collaborative),
* Evaluasi berbasis umpan balik pengguna nyata (user feedback).

Dengan penyempurnaan tersebut, sistem ini dapat menjadi solusi rekomendasi yang lebih komprehensif dan adaptif terhadap kebutuhan pengguna.

---
"""